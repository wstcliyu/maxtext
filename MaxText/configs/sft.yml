base_config: "base.yml"

use_sft: True
sft_train_on_completion: True
packing: True

per_device_batch_size: 1.0
max_prefill_predict_length: 512
max_target_length: 1024
eval_interval: 5  # test eval once, in the middle of 10 training steps
eval_steps: 2
learning_rate: 2.e-5

# HF pipeline
dataset_type: hf
# configurations for HuggingFaceH4/ultrachat_200k dataset
hf_path: 'HuggingFaceH4/ultrachat_200k'
train_split: 'train_sft'
hf_eval_split: 'test_sft'
train_data_columns: ['messages']
eval_data_columns: ['messages']
# configurations for trl-lib/ultrafeedback-gpt-3.5-turbo-helpfulness dataset
#hf_path: 'trl-lib/ultrafeedback-gpt-3.5-turbo-helpfulness'
#train_split: 'train'
#hf_eval_split: 'test'
#train_data_columns: ['prompt', 'completion']
#eval_data_columns: ['prompt', 'completion']

enable_goodput_recording: False
monitor_goodput: False
enable_checkpointing: True
