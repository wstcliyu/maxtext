import jax
from jax.sharding import Mesh
import jax.numpy as jnp
import numpy as np
from input_pipeline import input_pipeline_interface
from input_pipeline import _input_pipeline_utils
import functools
import datasets
import transformers
import grain.python as grain
from collections.abc import Iterable
from functools import partial
import jax.tree_util as jtu

class SingleHostDataLoader:
  def __init__(self, dataloader: grain.DataLoader, global_mesh: Mesh):
    self.global_mesh = global_mesh
    self.dataloader = dataloader
    if isinstance(self.dataloader, Iterable):
      self.local_iterator = iter(self.dataloader)
    else:
      raise ValueError("Type error: dataloader should be either tf.data.Dataset or Iterable.")

  def reset(self):
    if isinstance(self.dataloader, Iterable):
      self.local_iterator = iter(self.dataloader)
    else:
      raise ValueError("Type error: dataloader should be either tf.data.Dataset or grain.DataLoader.")

  def __iter__(self):
    self.reset()
    return self

  def __next__(self):
    local_data = next(self.local_iterator)
    # def _get_local_device_buffers(arr):
    #   try:
    #     local_device_arrays = np.split(arr, len(self.global_mesh.local_devices), axis=0)
    #   except ValueError as array_split_error:
    #     raise ValueError(
    #         f"Unable to put to devices shape {arr.shape} with "
    #         f"local device count {len(self.global_mesh.local_devices)} "
    #     ) from array_split_error
    #   return jnp.vstack(jax.device_put(local_device_arrays, self.global_mesh.local_devices))
    # local_device_buffers = jtu.tree_map(partial(_get_local_device_buffers), local_data)

    return local_data

def preprocessing_pipeline(dataloading_host_index,
    dataloading_host_count,
    global_mesh,
    dataset,
    data_column_names,
    tokenize,
    tokenizer_path,
    hf_access_token,
    global_batch_size,
    max_target_length,
    add_bos=True,
    add_eos=True,
    num_threads=1,
    drop_remainder=False,):
  """GRPO input pipeline for preprocessing HF dataset
  
    Return an iterator of dataset local to each host.
  """

  assert global_batch_size % global_mesh.size == 0, "Batch size should be divisible number of global devices."

  if tokenize:
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        tokenizer_path,
        add_bos_token=add_bos,
        add_eos_token=add_eos,
        model_max_length=max_target_length,
        legacy=False,
        token=hf_access_token,
    )

    dataset = dataset.map(
        _input_pipeline_utils.tokenization,
        batched=True,
        fn_kwargs={"hf_tokenizer": tokenizer, "max_length": max_target_length - 1, "column_names": data_column_names},
    )
  dataset = dataset.select_columns(data_column_names)
  dataset = _input_pipeline_utils.HFDataSource(
    dataset,
    dataloading_host_index,
    dataloading_host_count,
    num_threads,
    False,
    max_target_length,
    data_column_names,
  )
  operations = []
  lists2array = lambda x: jax.tree.map(np.asarray, x, is_leaf=lambda x: isinstance(x, (list, tuple)))
  operations.append(grain.MapOperation(lists2array))
  operations.append(_input_pipeline_utils.PadOrTrimToMaxLength(max_target_length))
  operations.append(grain.Batch(batch_size=global_batch_size // jax.process_count(), drop_remainder=drop_remainder))

  # Since HuggingFace IterableDataset does not support access through index
  # Indexes generated by dummy_index_sampler is not used.
  # dummy_index_sampler is used as an input place holder for grain.Dataloader
  dummy_index_sampler = grain.IndexSampler(
    num_records=len(dataset),
    num_epochs=1,
    shard_options=grain.ShardOptions(
        shard_index=dataloading_host_index, shard_count=dataloading_host_count, drop_remainder=False
    ),
    shuffle=False,
    seed=0,
  )

  dataloader = grain.DataLoader(
      data_source=dataset,
      operations=operations,
      sampler=dummy_index_sampler,
      worker_count=1,  # only supports one worker for now, more workers results in duplicated data
      worker_buffer_size=1,
      read_options=grain.ReadOptions(num_threads=num_threads, prefetch_buffer_size=128),
  )

  # single_host_gen = SingleHostDataLoader(dataloader, global_mesh)
  return iter(dataloader)



def make_hf_train_iterator(
    config,
    global_mesh,
    process_indices_train,
):
  """Load, preprocess dataset and return local iterators"""
  train_ds = datasets.load_dataset(
    config.hf_path,
    data_dir=config.hf_data_dir,
    data_files=config.hf_train_files,
    split="train",
    streaming=True,
    token=config.hf_access_token,
  )
  local_iter = preprocessing_pipeline(
    dataloading_host_index=process_indices_train.index(jax.process_index()),
      dataloading_host_count=len(process_indices_train),
      global_mesh=global_mesh,
      dataset=train_ds,
      data_column_names=config.train_data_columns,
      tokenize=config.tokenize_train_data,
      tokenizer_path=config.tokenizer_path,
      hf_access_token=config.hf_access_token,
      global_batch_size=config.global_batch_size_to_load,
      max_target_length=config.max_prefill_predict_length,
  )
  return local_iter

def create_data_iterator(config, mesh):
  process_indices_train = input_pipeline_interface.get_process_loading_real_data(
    config.data_sharding,
    config.global_batch_size_to_load,
    config.global_batch_size_to_train_on,
    config.max_target_length,
    mesh,
  )
  assert config.eval_interval <= 0, "GRPO input pipeline is not supported for eval data"
  train_iterator_fn = functools.partial(make_hf_train_iterator, config, mesh, process_indices_train)
  return input_pipeline_interface.make_mixed_iterator(config, mesh, process_indices_train, [], train_iterator_fn, None)
